+++
title = "蒙特卡洛树搜索"
date = 2025-11-05
description = "算法"

[taxonomies]
categories = ["algorithm"]
tags = ["algorithm"]
+++

## 蒙特卡洛树搜索

在看 [AFlow](https://openreview.net/forum?id=z5uVAKwmjf) 这篇论文的时候, 发现里面有提到蒙特卡洛树搜索 (MCTS) 这个算法。我对这个算法完全是一无所知，只是知道它是一种搜索算法，和其他搜索算法（遗传算法，巴拉巴拉什么的一样），都是寻找特定空间的最优解问题。

在和 Gemini 聊了很多轮后，我也是对 MCTS 这个算法有了一定的了解。它的基本流程可以简单地概括为利用+探索，拓展，模拟，反向传播。下面简单记录一下我对这个算法的理解，也顺带在输出的过程中，发现那些还有困惑的地方。


## 它所面临的场景：为何需要 MCTS？

在聊 MCTS 怎么做之前，我们得先聊聊**为什么**需要它。

传统的游戏 AI 算法，比如 Minimax（极小化极大算法），本质上是一种 DFS（深度优先搜索）。它试图暴力搜索整棵游戏树，评估所有可能的未来，然后找到一个绝对最优解。

对于五子棋，这或许还行。但对于围棋（Go）呢？

围棋的游戏树复杂度（所有可能的合法走法）比宇宙中的原子总数还多。你不可能算完它。

这就是 MCTS 要解决的核心问题：当搜索空间（所有可能的未来）过于庞大，以至于无法暴力算完时，你该如何做出一个足够好的甚至是最优的决策？

MCTS 的答案是：我放弃算尽一切，转而使用智慧的采样。我不会均匀地搜索所有路径，而是会把我的计算资源集中在那些看起来有前途的路径上。它用大数定律的统计学思想，来代替确定性的计算。只要我足够多的迭代，最终就能逐渐收敛到一个近似最优的解。

AlphaGo 成功的背后也有 MCTS 的身影。

## 算法流程

MCTS 的算法流程是一个不断迭代的循环，大致分为四步
1. 选择 (Selection)：从根节点出发，根据一个评分公式，一路向下，找到一个最值得深入的路径，直到抵达我们**已知树的边界**。
2. 拓展 (Expansion)：在**边界**处，添加一个新的、未访问过的子节点。
3. 模拟 (Simulation)：从这个新节点开始，随机快进下到底，看这一局是赢是输。即快速模拟当前节点的希望。
4. 反向传播：将赢/输的结果，传回给路径上的所有父节点，更新它们的统计数据。

这个循环会重复成千上万次（比如在 1 秒钟内）。时间到！停止循环。查看根节点的所有子节点，选择一个最稳健的作为最终走法。

### 选择

这是 MCTS 最核心、最巧妙的一步。它解决的问题：我应该在哪个已知的分支上投入更多的‘思考’？

它用一个 UCT(Upper Confidence Bound 1 applied to Trees，树的置信上限)公式完美地平衡了两个概念：
- 利用 (Exploitation)：选择那个当前看起来胜率最高的走法。
- 探索 (Exploration)：选择那个我还不太了解，但不确定性很高的走法（万一它是奇招呢？）。
> 写到这里的时候，我想起了之前学习强化学习，也有类似的利用x探索的策略，比如 Q learning 中的 epsilon-greedy 策略。

**UCT 分数 = (w_i / n_i) + C * sqrt(ln(N) / n_i)**

我们来拆解它：

- w_i / n_i (平均胜率)：这就是利用。w_i 是这个节点赢的次数，n_i 是它总共被访问的次数。这个值越高，说明它历史战绩越好。
- C * sqrt(ln(N) / n_i) (探索奖励)：这就是探索。
- N 是父节点被访问的总次数。
- n_i 还是这个子节点被访问的次数。
- 关键： 如果一个子节点 n_i 很小（我们很少试它），1 / n_i 就会很大，导致整个探索奖励飙升。这会强制算法去将资源倾斜到这个冷门选项。

选择阶段的完整流程是：

- 从根节点 R 开始。查看其所有已被访问过的子节点（A, B, C）。
- 使用 UCT 公式计算 A, B, C 的分数。
- 选择分数最高的那个（比如 B），跳到 B。
- 重复这个过程：在 B 节点，再用 UCT 公式选出它分数最高的子节点（D, E, F...），再跳过去...

这个过程何时停止？这个UCT 跳跃会一直持续，直到我们第一次撞到一个节点 P，它满足：
- P 本身是被访问过的。
- P 至少有一个子节点是未被访问的（n_child = 0）。

这个 P 节点，就是我们这条最值得路径的终点，也是我们已知树的边界。选择阶段结束， 我们把 P 节点交给下一步拓展。

### 拓展

因为选择阶段，我们已经选出了树的边界了，所以我们需要拓展这个边界。

拓展的过程很简单：在 P 节点，随机选择一个未被访问过的子节点（比如 I），并添加到 MCTS 树中。P 节点必然有一个未被访问的子节点，因为我们在选择阶段，选出来的 P 至少有一个子节点是未被访问的。

这个新节点 I 的初始数据是：w_I = 0，n_I = 0。现在，I 是我们这轮循环的主角。

### 模拟

我们刚创建了新节点 I，对它一无所知（0/0 没法用 UCT）。我们该如何快速知道它是好棋还是臭棋？

答案是：随机快进（也叫 Playout 或 Rollout）。

1. 从 I 节点代表的局面开始。
2. 算法随机地帮我方走一步。
3. 算法随机地帮对手走一步。
4. 重复这个随机互下的过程，直到游戏分出胜负（比如黑棋赢了）。
5. 记录下这个结果（比如 +1 代表赢，0 代表输）。

这就像石头剪刀布一样，单次看运气成分很大，但根据「大数定律」，只要我们模拟的次数足够多，这个粗略的估计就会非常接近真实胜率。

### 反向传播

我们拿到了上一步模拟的胜负结果（比如 +1）。

这一步的工作，就是把这个结果递归地更新回路径上的所有祖先节点。
1. 更新 I：w_I 变为 1，n_I 变为 1。
2. 更新 I 的父亲 P：w_P (总胜场) +1，n_P (总访问) +1。
3. 更新 P 的父亲...：w +1, n +1。
4. 一直更新到根节点 R：w_R +1, n_R +1。

至此，一个完整的 MCTS 循环结束。算法会立即从根节点 R 重新开始下一次循环。

### 最终决策：时间到了！

当 1 秒钟的思考时间用完，MCTS 循环停止。算法必须在棋盘上落子了。

它会回到根节点 R，查看它的所有子节点（A, B, C...），然后做出最终选择。

标准是什么？不是胜率最高，甚至也不是赢的次数最多，而是**访问次数 n 最高**的那个节点。

为什么？

为什么不是胜率 (w/n)？ * 节点 A: w=1, n=1 (胜率 100%)

- 节点 B: w=900, n=1000 (胜率 90%)
- 胜率会选 A，但 A 只是运气好的小样本事件，B 才是被 1000 次访问验证过的好棋。

为什么不是胜场 (w)？

- 节点 A: w=100, n=1000 (胜率 10%)
- 节点 B: w=90, n=100 (胜率 90%)
- 胜场会选 A，但 A 其实是被访问了 1000 次才赢 100 次的臭棋。

n (访问次数) 是最稳健的指标。一个节点的 n 很高，说明 MCTS 的 UCT 公式（在利用和探索的反复权衡中）最终决定将最多的计算资源投入给它。它是那个被算法千锤百炼后选出来的、最值得信赖的走法。

## AlphaGo 中的 MCTS：神经网络与作弊器

MCTS 算法很强，但它的模拟阶段（随机快进）又慢又蠢。如果能优化这个阶段，效率会大大提升。
AlphaGo 的做法就是用深度学习来替换MCTS 的短板。它不是只用一个作弊器，而是用了**两个**：

1.  **价值网络 (Value Network)**：
    * **替换了模拟 (第 3 步)**。
    * MCTS 不再需要随机快进到底了。当它拓展出一个新节点 `I` 时，它直接把 `I` 的局面丢给价值网络。
    * 这个网络会**瞬间**给出一个胜率估计（比如 `0.73`）。这个结果比随机快进1000 次得到的 `+1` 或 `0` 要**准确得多**。

2.  **策略网络 (Policy Network)**：
    * **优化了拓展 (第 2 步)**。
    * MCTS 在拓展节点 `P` 时，不再是随机挑一个子节点了。
    * 它会把 `P` 局面丢给策略网络，这个网络会返回一个概率分布，告诉你：`P` 的 10 个下一步里，走 `I` 的可能性是 50%，走 `J` 的可能性是 30%...
    * MCTS 会**优先**去拓展那些策略网络推荐的、看起来更像好棋的节点。

AlphaGo = MCTS (作为总指挥框架) + 两个神经网络 (作为专家顾问)。
它把 MCTS 的统计学智慧和 深度学习的模式识别直觉完美地结合在了一起。

## 总结

等几天后，再回过头来复习看下，看能忘多少...